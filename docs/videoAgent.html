<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles/styles-videoAgent.css" />
  <title>videoAgent</title>
</head>
<body>
  <div class="main-container">
    <h1>MMCT - Video Pipeline</h1>
    
    <div class="small-intro"> 
      Extracts transcript & visual summary, fetches relevant timestamp using the cosine similar transcript or summary, Calls Azure Vision API to fetch relevant timestamp of query, Uses GPT-4V for query the frames on the provided timestamp with the query.
    </div>
    <div class="addImage">
        <img src="Images&icons/VideoPipeline.jpeg" alt="imageAgent">
        <p class="caption">fig3. MMCT VideoAgent Pipeline</p>
    </div>

    <div class="overview">
      <h2 class="underlined">Overview</h2>
      <p>Video Pipeline consists of two components:</p>
        <ul>
          <li id="ingestion-required">Video Ingestion</li>
          <li>Video Agent</li>
        </ul>
      <p>
        You can first ingest your specific video using the Video Ingestion pipeline, which processes and prepares the video content for downstream tasks. Once ingested, the <strong>Video Agent</strong> can be used to perform Question Answering over the video. It leverages multiple tools to accurately extract relevant information and optionally uses a <strong>Critic Agent</strong> (if enabled) to enhance the quality and precision of the response.
      </p>
    </div>
    <div class="tool-config-section">
      <h2 class="underlined">1. Video Ingestion<span class="tooltip"> *<span class="tooltip-text">Prerequisite for videoAgent</span></span></h2>
      <p>
        The <strong>IngestionPipeline</strong> performs comprehensive processing of video file to extract transcript, frames, chapters, AI Search index creation for downstream applications like <code>VideoAgent</code> 
        (refer <code>fig4</code>).
      </p>

      <div class="addpipeline">
        <img src="Images&icons/ingestionpipeline.png" alt="">
        <p class="caption">fig4. MMCT Video Ingestion Pipeline</p>
      </div>

      <p>It includes the following steps:</p>
      <ul>
        <li><strong>Audio Extraction</strong> – Extracts the audio from the input video.</li>
        <li><strong>Transcription</strong> – Converts spoken content to text using the selected transcription service and language setting.</li>
      </ul>
      <div class="note">
        <strong>Transcription Configuration:</strong><br>
        You can configure the transcription backend using the <code>TranslationServices</code> enum:
        <ul>
          <li><code>TranslationServices.WHISPER</code> – Uses OpenAI Whisper.</li>
          <li><code>TranslationServices.AZURE_STT</code> – Uses Azure Speech-to-Text.</li>
        </ul>
        Specify the language of the video's audio using the <code>Languages</code> enum. For example:
        <ul>
          <li><code>Languages.ENGLISH_INDIA</code></li>
          <li><code>Languages.HINDI</code></li>
        </ul>
      </div>
      <ul>
        <li><strong>Frame Extraction</strong> – Captures representative frames at 1 FPS intervals to support visual summarization and downstream VideoAgent.</li>
        <li><strong>Chapter Generation</strong> – Aligns transcript segments with visual frames to form meaningful video chapters.</li>
        <li><strong>Azure Search Indexing</strong> – Saves chapters and metadata to an Azure AI Search index to support retrieval.</li>
        <li><strong>Summary File Generation</strong> – Outputs <code>summary_n_transcript.json</code> containing the full transcript and a visual summary.</li>
        <li><strong>(Optional) Azure CV Indexing</strong> – Optionally indexes the video frames using Azure Computer Vision for advanced content-based search.</li>
      </ul>
    </div>

    <div class="usage">
      <h2 class="underlined">2. Video Agent</h2>
      <p>
        <strong>VideoAgent</strong> operates in two key stages:
      </p>
      <ul>
        <li><strong>Video Retrieval</strong> – Retrieves relevant videos from a pre-ingested <code>Azure AI Search index </code>.</li>
        <li><strong>Video Question Answering (QA)</strong> – Uses the <strong>MMCT</strong> framework to answer questions (refer fig5).
          <ul>
            <li><strong>Planner:</strong> Drives the reasoning using a structured toolchain.</li>
            <li><strong>Critic (optional):</strong> Evaluates and improves the planner’s output.</li>
          </ul>
          <div class="addImage">
            <img src="Images&icons/videoAgent.png" alt="imageAgent">
            <p class="caption">fig5. Video Question Answering</p>
        </div>
        </li>
      </ul>
      <div class="note">
        <strong>Note:</strong> The critic agent is enabled by default. Disable with <code>use_critic_agent=False</code>. Skipping this may reduce accuracy.
      </div>
    </div>

    <div class="usage">
      <h2 class="underlined">Tool Workflow</h2>
      <p><strong>VideoAgent</strong> uses a fixed tool pipeline for QA:</p>
      <ul>
        <li><code>GET_SUMMARY_TRANSCRIPT</code> – Retrieves transcript and visual summary.</li>
        <li><code>QUERY_SUMMARY_TRANSCRIPT</code> – Identifies 3 relevant timestamps in transcript.</li>
        <li><code>QUERY_AZURE_COMPUTER_VISION</code> – (optional) Finds 3 visual timestamps via Azure CV.</li>
        <li><code>QUERY_GPT4V</code> – Uses GPT-4V for multimodal reasoning over selected frames.</li>
      </ul>
      <p>
        All tools are used by default. To skip Azure CV, set <code>use_azure_cv_tool=False</code>.
      </p>
    </div>

    <div class="usage">
      <h2 class="underlined">Usage</h2>
      <p><strong>MMCT Video Ingestion</strong></p>
      <pre class="code-block"><code>
  import asyncio
  from mmct.video_pipeline import IngestionPipeline, Languages, TranscriptionServices

  video_path = ""
  index = ""
  source_language = Languages.ENGLISH_INDIA

  ingestion = IngestionPipeline(
      video_path=video_path,
      index_name=index,
      transcription_service=TranscriptionServices.AZURE_STT,
      language=source_language,
  )

  asyncio.run(ingestion())
  </code></pre>

        <p><strong>MMCT Video Agent</strong></p>
        <pre class="code-block"><code>
  import asyncio
  import ast
  from mmct.video_pipeline import VideoAgent

  query = ""
  index_name = ""
  top_n = 3
  use_azure_cv_tool = False
  use_critic_agent = True
  stream = True

  video_agent = VideoAgent(
      query=query,
      index_name=index_name,
      top_n=top_n,
      use_azure_cv_tool=use_azure_cv_tool,
      use_critic_agent=use_critic_agent,
      stream=stream,
  )

  response = asyncio.run(video_agent())
  print(response)
  
</code></pre>
    </div>

    <div class="qna">
      <h2 class="underlined">QnA</h2>
    </div>

    <!-- License -->
    <div class="license">
        <h2 class="underlined">License</h2>
        <div class="section-box">
            <p>Microsoft's autogen</p>
        </div>
    </div>
  </div>
</body>
</html>
