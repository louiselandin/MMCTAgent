{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c13ffc",
   "metadata": {},
   "source": [
    "## Example Notebook: Using VideoAgent\n",
    "\n",
    "### ðŸ› ï¸ Setup Instructions\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "- Ensure you have created an `.env` file in the **same directory** as this notebook. It must contain all required environment variables (e.g., API keys or configuration values expected by `VideoAgent`).\n",
    "- Make sure all required libraries are installed by running:\n",
    "\n",
    "  ```bash\n",
    "  pip install -r requirements.txt\n",
    "  ```\n",
    "\n",
    "### About\n",
    "\n",
    "**VideoAgent** operates in two key stages:\n",
    "\n",
    "1. **Video Retrieval** â€“ Given a user query, the agent retrieves relevant videos from a pre-ingested **Azure AI Search index**. This search ensures that only contextually relevant videos are passed on for deep analysis.\n",
    "\n",
    "2. **Video Question Answering (QA)** â€“ After retrieval, the agent uses the **Multi-Modal Critical Thinking (MMCT)** framework ([arxiv.org/abs/2405.18358](https://arxiv.org/abs/2405.18358)) to generate a high-quality answer. MMCT involves two agents:\n",
    "\n",
    "   - **Planner**: Drives the reasoning process using a structured toolchain, generating an initial response.\n",
    "   - **Critic (optional)**: Analyzes the plannerâ€™s output and, if needed, provides feedback that prompts an improved final answer.\n",
    "\n",
    "> **Note:** The critic agent is enabled by default. You can disable it by setting `use_critic_agent=False` during initialization.  \n",
    "> **Disabling the critic agent skips the feedback loop and may reduce the accuracy of the final response.**\n",
    "\n",
    "---\n",
    "\n",
    "### Tool Workflow\n",
    "\n",
    "Unlike independent tool selection, **VideoAgent uses a fixed pipeline** of tools that work collaboratively during the QA stage. These tools are automatically orchestrated by the planner:\n",
    "\n",
    "- `GET_SUMMARY_TRANSCRIPT` â€“ Extracts the full transcript and a high-level visual summary of the video.\n",
    "- `QUERY_SUMMARY_TRANSCRIPT` â€“ Given a query, this tool identifies **three timestamps** in the transcript that are most relevant.\n",
    "- `QUERY_AZURE_COMPUTER_VISION` _(optional)_ â€“ Uses **Azure Computer Vision** to return **three additional timestamps** related to the visual content of the query.\n",
    "- `QUERY_GPT4V` â€“ Uses **OpenAI's GPT-4V** to inspect video frames around the identified timestamps and generate a detailed response grounded in both visual and textual understanding.\n",
    "\n",
    "By default, all tools are used in a coordinated pipeline. You can disable **only** the Azure Computer Vision tool by setting `use_azure_cv_tool=False` during agent initialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128f1ba",
   "metadata": {},
   "source": [
    "### Importing Libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb122cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import ast\n",
    "from mmct.video_pipeline import VideoAgent\n",
    "\n",
    "# nest_asyncio is used to allow nested event loops in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53503ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the steps involved in applying Beejamrutham to seeds?  -  Answer in the transcript file itself\n",
    "# What local ingredients are used in the preparation of Beejamrutham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to run the VideoAgent class\n",
    "query = \"What local ingredients are used in the preparation of Beejamrutham?\"\n",
    "index_name = \"telangana-video-index-latest-2\"  # Azure AI Search index name\n",
    "top_n = 2  # Number of top results (video ids for MMCT VQnA) to return from the index\n",
    "use_azure_cv_tool = False\n",
    "use_critic_agent = True\n",
    "stream = True\n",
    "\n",
    "video_agent = VideoAgent(\n",
    "    query=query,\n",
    "    index_name=index_name,\n",
    "    top_n=top_n,\n",
    "    use_azure_cv_tool=use_azure_cv_tool,\n",
    "    use_critic_agent=use_critic_agent,\n",
    "    stream=stream,\n",
    ")\n",
    "\n",
    "response = asyncio.run(video_agent())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8401d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b22f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmct_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
